defaults:
  - _self_
  - experiment: baseline
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog

# Dataset parameters (fixed, not swept)
dataset:
  code_type: "surface_code" # "surface_code" or "bivariate_bicycle"
  
  # Surface code parameters (used when code_type="surface_code")
  d: 9                     # Surface code distance
  mwpm_filter: false       # Enable/disable MWPM filtering
  chunking: [1,1,1]        # Chunk by time, x, and y dimension
  
  # Bivariate bicycle parameters (used when code_type="bivariate_bicycle")
  l: 6                     # BB code parameter l
  m: 6                     # BB code parameter m
  
  # Common parameters
  p: 2.1                   # Error probability parameter
  rounds_list: [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]  # List of rounds to sample from

# Model architecture (main sweep targets)
model:
  architecture: resnet50   # resnet18/34/50/101/152 OR rgcn18/34/50/101/152
  embedding_dim: 64        # Embedding dimension
  channel_multipliers: [2, 4, 8, 16]  # Channel expansion per stage [stage1, stage2, stage3, stage4]
                                       # [2,4,8,16] = standard ResNet (default)
                                       # [1,1,1,1] = constant channels (memory efficient)
                                       # [1,1.5,2,2.5] = gradual growth
  
  # ResNet3D specific parameters (ignored for RGCN)
  stage3_stride: [1,1,1]   # Stage 3 downsampling stride (T,H,W)
  stage4_stride: [1,1,1]   # Stage 4 downsampling stride (T,H,W)
  use_lstm: false          # LSTM for temporal processing
  
  # RGCN specific parameters (ignored for ResNet3D)
  # num_relations: auto-inferred from dataset

# Training parameters
training:
  batch_size: null       # Manual batch size override (null = auto-tune)
  lr: 3e-4               # Learning rate (main sweep target)
  weight_decay: 0.0      # Weight decay for regularization (reasonable range: 1e-4 to 5e-4)
  optimizer: schedulefree       # Optimizer choice: adamw or schedulefree
  max_steps: 50000       # Maximum training steps
  log_every_n_steps: 100 # Logging frequency
  checkpoint_every_minutes: 15  # Checkpoint frequency in minutes
  precision: bf16-mixed  # Mixed precision training
  gradient_clip_val: 1.0 # Gradient clipping value (null = no clipping)
  gradient_clip_algorithm: norm  # Gradient clipping algorithm: norm or value
  accumulate_grad_batches: 1  # Gradient accumulation batches (1 = no accumulation)

# Hardware/performance settings
hardware:
  accelerator: auto      # Training accelerator
  devices: auto          # Number of GPUs (auto, or specific number like 2, 4, etc.)
  strategy: auto         # Training strategy (auto, ddp, ddp_find_unused_parameters_true, etc.)
  num_nodes: 1           # Number of nodes for multi-node training
  sync_batchnorm: true   # Synchronize batch norm across GPUs
  num_workers: 8         # DataLoader workers
  prefetch_factor: 4     # DataLoader prefetch
  persistent_workers: true  # Keep workers alive

# Experiment settings
experiment:
  name: resnet_experiment  # Experiment name for logging
  base_dir: /n/netscratch/yelin_lab/Everyone/andigu/scaling  # Base directory for all experiment data

# 3-stage curriculum learning settings
curriculum:
  enabled: false            # Enable 3-stage curriculum learning
  stage1_p: 0.5             # Low noise level for stage 1
  stage1_steps: 25000       # Steps in stage 1
  stage2_p_end: 2.1         # Target p for curriculum (stage 2 starts from stage1_p)
  stage2_steps: 25000       # Steps for curriculum annealing
  stage3_steps: 25000       # Steps at high noise (final training)
  # Total training = stage1_steps + stage2_steps + stage3_steps = 75000 steps

# Weights & Biases settings
wandb:
  enabled: true             # Enable/disable wandb logging
  project: scaling  # W&B project name
  entity: null              # W&B entity (team), null uses default
  tags: [resnet, surface_code, decoder]  # Tags for organizing runs
