{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aee9b33-9491-4669-8711-0c4a8af92d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a801963b-816c-487d-ab01-c725d6c85762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulate import Algorithm, BivariateBicycle, NoiseModel, CodeFactory, StimSimulator, LogicalCircuit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "from torch_geometric.data import Data\n",
    "import math\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "class BivariateBicycleDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    PyTorch IterableDataset for bivariate bicycle code error correction data.\n",
    "    \n",
    "    Generates batches of bivariate bicycle code detector measurements and corresponding \n",
    "    logical error labels for training graph neural network decoders.\n",
    "    \n",
    "    Supports 3-stage curriculum learning with automatic p adjustment.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, l=6, m=6, rounds_max=9, p=2.0, batch_size=32, \n",
    "                 stage_manager=None, num_workers=8, global_step_offset=0, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            l: BB code parameter l\n",
    "            m: BB code parameter m\n",
    "            rounds_max: Maximum number of error correction rounds\n",
    "            p: Default error probability parameter (used when stage_manager is None)\n",
    "            batch_size: Batch size for generated data\n",
    "            stage_manager: StageManager instance for curriculum learning (optional)\n",
    "            num_workers: Total number of DataLoader workers (for step estimation)\n",
    "            global_step_offset: Starting global step offset (for resuming)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.default_p = p\n",
    "        self.batch_size = batch_size\n",
    "        self.l = l\n",
    "        self.m = m\n",
    "        self.rounds_max = rounds_max\n",
    "        self.stage_manager = stage_manager\n",
    "        self.num_workers = num_workers\n",
    "        self.global_step_offset = global_step_offset\n",
    "        \n",
    "        # Track local worker sample count (will be set in each worker)\n",
    "        self.local_sample_count = 0\n",
    "        \n",
    "        # Build static code graph structure (independent of rounds)\n",
    "        self._static_graph_structure = self._build_static_graph_structure()\n",
    "    \n",
    "    def get_num_relations(self):\n",
    "        \"\"\"Get the total number of relation types (static + temporal).\"\"\"\n",
    "        num_static_edge_types = len(self._static_graph_structure['all_edge_types'])\n",
    "        return num_static_edge_types * 3  # 3 for temporal connections [-1, 0, 1]\n",
    "    \n",
    "    def get_num_logical_qubits(self):\n",
    "        \"\"\"Get the number of logical qubits encoded by this BB code.\"\"\"\n",
    "        # logical_operators is a numpy structured array with fields: logical_id, logical_pauli, data_id, physical_pauli\n",
    "        # logical_id goes from 0 to k-1 where k is the number of logical qubits\n",
    "        logical_ops = self._static_graph_structure['metadata'].logical_operators\n",
    "        return np.max(logical_ops['logical_id']) + 1\n",
    "    \n",
    "    def _estimate_global_step(self) -> int:\n",
    "        \"\"\"Estimate current global step from worker-local sample count.\"\"\"\n",
    "        return self.global_step_offset + (self.local_sample_count * self.num_workers)\n",
    "    \n",
    "    def get_current_p(self) -> float:\n",
    "        \"\"\"Get current p value based on stage manager or default.\"\"\"\n",
    "        if self.stage_manager is not None:\n",
    "            estimated_step = self._estimate_global_step()\n",
    "            return self.stage_manager.get_current_p(estimated_step)\n",
    "        else:\n",
    "            return self.default_p\n",
    "    \n",
    "    def _build_static_graph_structure(self):\n",
    "        \"\"\"Build the static BB code graph structure (independent of rounds).\"\"\"\n",
    "        # Create BB code metadata\n",
    "        cf = CodeFactory(BivariateBicycle, {'l': self.l, 'm': self.m})\n",
    "        metadata = cf().metadata\n",
    "        \n",
    "        # Build tanner graph - this creates edges between checks and data\n",
    "        tanner = pd.DataFrame(metadata.tanner)\n",
    "        \n",
    "        # Create check-to-check graph as in bb-test.ipynb\n",
    "        c2c = tanner.merge(tanner, on='data_id')[['check_id_x', 'check_id_y', 'edge_type_x', 'edge_type_y']]\n",
    "        all_edge_types = []\n",
    "        graph_edges = []\n",
    "        \n",
    "        for (c1, c2), df in c2c.groupby(['check_id_x', 'check_id_y']):\n",
    "            edge_types = df[['edge_type_x', 'edge_type_y']].to_numpy()\n",
    "            edge_types = frozenset([tuple(map(int, x)) for x in edge_types])\n",
    "            if edge_types not in all_edge_types:\n",
    "                all_edge_types.append(edge_types)\n",
    "            c1, c2 = map(int, (c1, c2))\n",
    "            graph_edges.append((c1, c2, all_edge_types.index(edge_types)))\n",
    "        \n",
    "        graph_df = pd.DataFrame(graph_edges, columns=['syndrome_id', 'neighbor_syndrome_id', 'edge_type'])\n",
    "        \n",
    "        return {\n",
    "            'graph_df': graph_df,\n",
    "            'all_edge_types': all_edge_types,\n",
    "            'metadata': metadata\n",
    "        }\n",
    "    \n",
    "    def _build_spatiotemporal_graph_structure(self, rounds, mt):\n",
    "        \"\"\"Build the spatiotemporal graph structure using the static graph and temporal connections.\n",
    "        \n",
    "        Args:\n",
    "            rounds: Number of error correction rounds\n",
    "            mt: MeasurementTracker from the LogicalCircuit (contains detector metadata)\n",
    "        \"\"\"\n",
    "        # Use pre-computed static structure\n",
    "        graph_df = self._static_graph_structure['graph_df']\n",
    "        all_edge_types = self._static_graph_structure['all_edge_types']\n",
    "        \n",
    "        # Get actual detector metadata from measurement tracker\n",
    "        det = pd.DataFrame(mt.detectors)\n",
    "        \n",
    "        det_graph = det.merge(graph_df, on='syndrome_id').merge(\n",
    "            det, left_on='neighbor_syndrome_id', right_on='syndrome_id', suffixes=('', '_nb')\n",
    "        )\n",
    "        \n",
    "        # Filter temporal connections (allow connections within 1 time step)\n",
    "        det_graph = det_graph[np.abs(det_graph['time'] - det_graph['time_nb']) <= 1]\n",
    "        \n",
    "        # Encode temporal information in edge types\n",
    "        dt = det_graph['time_nb'] - det_graph['time'] + 1  # Maps [-1, 0, 1] to [0, 1, 2]\n",
    "        det_graph['final_edge_type'] = dt * len(all_edge_types) + det_graph['edge_type']\n",
    "        \n",
    "        # Create final edge list\n",
    "        det_graph = det_graph.sort_values(by=['detector_id', 'detector_id_nb', 'final_edge_type'])\n",
    "        edge_index = det_graph[['detector_id', 'detector_id_nb']].values.T\n",
    "        edge_attr = det_graph['final_edge_type'].values\n",
    "        \n",
    "        return {\n",
    "            'edge_index': edge_index,\n",
    "            'edge_attr': edge_attr,\n",
    "            'num_nodes': len(det),\n",
    "            'num_edge_types': (len(all_edge_types) * 3),  # 3 for temporal [-1, 0, 1]\n",
    "            'det_df': det,\n",
    "            'nb_counts': det_graph.groupby('detector_id')['detector_id_nb'].count().values\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_batch(l, m, rounds_max, p, batch_size):\n",
    "        \"\"\"Generate a single batch of bivariate bicycle code data.\"\"\"\n",
    "        rounds = np.random.randint(1, rounds_max + 1)\n",
    "        batch_size = math.floor(batch_size * rounds_max / rounds)\n",
    "        \n",
    "        # Create BB code circuit\n",
    "        alg = Algorithm.build_memory(cycles=rounds)\n",
    "        cf = CodeFactory(BivariateBicycle, {'l': l, 'm': m})\n",
    "        lc, _, mt = LogicalCircuit.from_algorithm(alg, cf)\n",
    "        noise_model = NoiseModel.get_scaled_noise_model(p).without_loss()\n",
    "        sim = StimSimulator(alg, noise_model, cf, seed=np.random.randint(0, 2**48))\n",
    "        \n",
    "        # Sample syndrome data\n",
    "        results = sim.sample(shots=batch_size)\n",
    "        detectors = results.detectors\n",
    "        logical_errors = results.logical_errors\n",
    "        \n",
    "        return detectors, logical_errors, (rounds, p), mt\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Generate infinite stream of bivariate bicycle code data batches.\"\"\"\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is not None:\n",
    "            worker_id = worker_info.id if hasattr(worker_info, 'id') else 'unknown'\n",
    "        else: \n",
    "            worker_id = 'main'\n",
    "            np.random.seed(0)\n",
    "        \n",
    "        # Reset local sample count for this worker\n",
    "        self.local_sample_count = 0\n",
    "        \n",
    "        while True:\n",
    "            # Calculate current p based on estimated global step\n",
    "            current_p = self.get_current_p()\n",
    "            \n",
    "            # Generate batch with current curriculum p value\n",
    "            detectors, logical_errors, (rounds, p), mt = self.generate_batch(\n",
    "                self.l, self.m, self.rounds_max, current_p, self.batch_size\n",
    "            )\n",
    "            graph_structure = self._build_spatiotemporal_graph_structure(rounds, mt)\n",
    "            edge_indices, edge_types = graph_structure['edge_index'], graph_structure['edge_attr'] # (2, |E|), (|E|, )\n",
    "            batch_size, num_nodes = detectors.shape\n",
    "            edge_indices = (edge_indices[..., None] + np.arange(batch_size)[None, None, :] * num_nodes).reshape((2, -1)).astype(np.int32)\n",
    "            edge_types = np.repeat(edge_types[:, None], batch_size, axis=1).reshape(-1).astype(np.int32)\n",
    "            detectors = detectors.astype(np.int32)\n",
    "            yield (detectors, edge_indices, edge_types), np.squeeze(logical_errors.astype(np.float32), axis=1), (rounds, p)\n",
    "\n",
    "            # Increment local sample count after generating batch\n",
    "            self.local_sample_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b96fccba-c534-40da-83d4-4dcec68b27a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.cuda.memory_allocated: 0.000166GB\n",
      "torch.cuda.memory_reserved: 0.001953GB\n",
      "torch.cuda.max_memory_reserved: 0.001953GB\n",
      "torch.Size([10, 100]) torch.float32\n",
      "tensor(0.6359, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.cuda.memory_allocated: 0.008493GB\n",
      "torch.cuda.memory_reserved: 0.021484GB\n",
      "torch.cuda.max_memory_reserved: 0.021484GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3686851/3156490106.py:28: UserWarning: Sparse BSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  bsr = torch.sparse_bsr_tensor(crow, edge_indices[1], values=matrices, requires_grad=True) # (out_nodes*out_dim) x (in_nodes*in_dim)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "ds = BivariateBicycleDataset(l=3, m=3, rounds_max=1)\n",
    "batch_size = 1\n",
    "detectors, logical_errors, (rounds, p), mt = ds.generate_batch(\n",
    "    ds.l, ds.m, ds.rounds_max, 1.0, batch_size\n",
    ")\n",
    "print(rounds)\n",
    "graph_structure = ds._build_spatiotemporal_graph_structure(rounds, mt)\n",
    "edge_indices, edge_types = graph_structure['edge_index'], graph_structure['edge_attr'] # (2, |E|), (|E|, )\n",
    "nb_counts = graph_structure['nb_counts']\n",
    "embedding_dim = 12\n",
    "num_nodes = detectors.shape[1]\n",
    "\n",
    "# emb = nn.Parameter(torch.randn(ds.get_num_relations(), embedding_dim, embedding_dim, device='cuda'))\n",
    "emb = nn.Embedding(ds.get_num_relations(), embedding_dim*embedding_dim, device='cuda', sparse=True)\n",
    "node_features = torch.randn((100, num_nodes, embedding_dim), device='cuda', dtype=torch.float32).flatten(start_dim=1).T\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "edge_indices, edge_types, nb_counts = [torch.from_numpy(x).cuda().int() for x in [edge_indices, edge_types, nb_counts]]\n",
    "# matrices = emb[edge_types]\n",
    "matrices = emb(edge_types).view(-1, embedding_dim, embedding_dim)\n",
    "\n",
    "crow = F.pad(torch.cumsum(nb_counts, dim=0), pad=(1,0))\n",
    "bsr = torch.sparse_bsr_tensor(crow, edge_indices[1], values=matrices, requires_grad=True) # (out_nodes*out_dim) x (in_nodes*in_dim)\n",
    "bsr = torch.randn((10, node_features.shape[0]), requires_grad=True, device='cuda').to_sparse()\n",
    "\n",
    "# ret = bsr @ node_features\n",
    "ret = torch.sparse.mm(bsr, node_features)\n",
    "loss = ret.mean()\n",
    "print(ret.shape, ret.dtype)\n",
    "print(loss)\n",
    "loss.backward()\n",
    "\n",
    "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bff38ea0-241c-4b54-a6a6-cacd7da4bc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 22.86 GiB. GPU 0 has a total capacity of 19.62 GiB of which 5.11 GiB is free. Including non-PyTorch memory, this process has 14.40 GiB memory in use. Of the allocated memory 12.48 GiB is allocated by PyTorch, and 1.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m     loss.backward()\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m emb.grad\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mf\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     36\u001b[39m loss = ret.mean()\n\u001b[32m     37\u001b[39m     \u001b[38;5;66;03m# print(ret.shape, ret.dtype)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m emb.grad\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tmp/algorithms-decoder/.venv/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tmp/algorithms-decoder/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tmp/algorithms-decoder/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 22.86 GiB. GPU 0 has a total capacity of 19.62 GiB of which 5.11 GiB is free. Including non-PyTorch memory, this process has 14.40 GiB memory in use. Of the allocated memory 12.48 GiB is allocated by PyTorch, and 1.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler\n",
    "import torch.nn.functional as F\n",
    "ds = BivariateBicycleDataset(l=6, m=6, rounds_max=9)\n",
    "batch_size = 1\n",
    "detectors, logical_errors, (rounds, p), mt = ds.generate_batch(\n",
    "    ds.l, ds.m, ds.rounds_max, 1.0, batch_size\n",
    ")\n",
    "\n",
    "graph_structure = ds._build_spatiotemporal_graph_structure(rounds, mt)\n",
    "edge_indices, edge_types = graph_structure['edge_index'], graph_structure['edge_attr'] # (2, |E|), (|E|, )\n",
    "nb_counts = graph_structure['nb_counts']\n",
    "embedding_dim = 64\n",
    "num_nodes = detectors.shape[1]\n",
    "\n",
    "node_features = torch.randn((batch_size, num_nodes, embedding_dim), device='cuda', dtype=torch.float32).flatten(start_dim=1).T\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "edge_indices, edge_types, nb_counts = [torch.from_numpy(x).cuda().int() for x in [edge_indices, edge_types, nb_counts]]\n",
    "# matrices = emb[edge_types]\n",
    "\n",
    "def f():\n",
    "    emb = nn.Parameter(torch.randn(ds.get_num_relations(), embedding_dim*embedding_dim, dtype=torch.float32, device='cuda'), requires_grad=True)\n",
    "    # with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "    matrices = emb[edge_types].flatten()\n",
    "    row_index = ((edge_indices[0] * embedding_dim)[:,None] + torch.arange(embedding_dim, device=edge_indices.device, dtype=torch.int32)[None,:])\n",
    "    col_index = ((edge_indices[1] * embedding_dim)[:,None] + torch.arange(embedding_dim, device=edge_indices.device, dtype=torch.int32)[None,:])\n",
    "    row_index = row_index[...,None].expand((-1,-1,embedding_dim)).flatten()\n",
    "    col_index = col_index[:,None,:].expand((-1,embedding_dim,-1)).flatten()\n",
    "\n",
    "    s = torch.sparse_coo_tensor(torch.stack([row_index, col_index], dim=0), matrices, (num_nodes*embedding_dim, num_nodes*embedding_dim), requires_grad=True, device='cuda')\n",
    "    print(s.dtype, node_features.dtype)\n",
    "    ret = s @ node_features\n",
    "\n",
    "    loss = ret.mean()\n",
    "        # print(ret.shape, ret.dtype)\n",
    "    loss.backward()\n",
    "    return emb.grad\n",
    "f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "918f3a66-91f7-40b1-84bd-5fa496f22bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "to(options) doesn't support converting to a different layout, but got self.layout being Strided and options.layout set as SparseCsr",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlprun\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-f f f()\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tmp/algorithms-decoder/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2504\u001b[39m, in \u001b[36mInteractiveShell.run_line_magic\u001b[39m\u001b[34m(self, magic_name, line, _stack_depth)\u001b[39m\n\u001b[32m   2502\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mlocal_ns\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.get_local_scope(stack_depth)\n\u001b[32m   2503\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m-> \u001b[39m\u001b[32m2504\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2506\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2507\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2508\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2509\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tmp/algorithms-decoder/.venv/lib/python3.12/site-packages/line_profiler/ipython_extension.py:130\u001b[39m, in \u001b[36mLineProfilerMagics.lprun\u001b[39m\u001b[34m(self, parameter_s)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m         \u001b[43mprofile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrunctx\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_ns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m         message = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mSystemExit\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tmp/algorithms-decoder/.venv/lib/python3.12/site-packages/line_profiler/line_profiler.py:185\u001b[39m, in \u001b[36mLineProfiler.runctx\u001b[39m\u001b[34m(self, cmd, globals, locals)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;28mself\u001b[39m.enable_by_count()\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28mself\u001b[39m.disable_by_count()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mf\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     36\u001b[39m     loss = ret.mean()\n\u001b[32m     37\u001b[39m     \u001b[38;5;66;03m# print(ret.shape, ret.dtype)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m emb.grad\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tmp/algorithms-decoder/.venv/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tmp/algorithms-decoder/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tmp/algorithms-decoder/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: to(options) doesn't support converting to a different layout, but got self.layout being Strided and options.layout set as SparseCsr"
     ]
    }
   ],
   "source": [
    "%lprun -f f f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "91ef31ac-b8c9-4279-9320-63a4b9d25538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420 ms ± 45.7 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7c310a3-f965-48c0-bb4e-07e0c8d27ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3855, 0.2998, 0.3855])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3,2).half().cuda()\n",
    "i = torch.LongTensor([[0, 1, 1],  [2, 0, 2]])\n",
    "v = torch.tensor([3., 4, 5], requires_grad=True)\n",
    "b = torch.sparse_coo_tensor(i, v, (2,3), dtype=torch.float16, device='cuda')\n",
    "b = b.to_sparse_csr()\n",
    "loss = (b @ a).mean()\n",
    "loss.backward()\n",
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fe628705-5c33-4574-828b-439c0d79efcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -4.6922,  -39.6140,   58.7204,  ...,   26.2005, -101.5606,\n",
       "          -41.4297],\n",
       "        [  14.5221,   32.1038,  -86.0991,  ...,   -2.1203,  -39.3223,\n",
       "          -47.0401],\n",
       "        [  32.7616,   47.8360,  120.8648,  ...,   76.2989,   22.3357,\n",
       "           34.6215],\n",
       "        ...,\n",
       "        [  31.4464,  -60.8588,  -49.7596,  ...,   -4.3467,    0.6159,\n",
       "          -49.7129],\n",
       "        [  12.2511,  -44.5404,   28.3795,  ...,   -3.8902,  -36.0490,\n",
       "          -10.4649],\n",
       "        [   2.3126,  -11.4291,  -30.5819,  ...,   -4.5334,  -11.0951,\n",
       "          -65.2871]], device='cuda:0', grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1d9ded8-7e02-4182-8f10-f22ff822cd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x154c1c92d970>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(to_bsr.to_dense().cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd60c61d-ed00-4acf-b321-40a95c76c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# plt.imshow(s.to_dense().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34e4f4bd-95cb-47b8-8dcd-63a7bfe18d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.1 μs ± 149 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit torch.matmul(to_bsr, node_features.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b074f140-a0d2-46ed-9543-0551292214b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "derivative for aten::sparse_compressed_tensor is not implemented",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[114]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# s = s.to_dense()\u001b[39;00m\n\u001b[32m     12\u001b[39m loss = torch.sparse.mm(s, v2).sum()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tmp/algorithms-decoder/.venv/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tmp/algorithms-decoder/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tmp/algorithms-decoder/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: derivative for aten::sparse_compressed_tensor is not implemented"
     ]
    }
   ],
   "source": [
    "i = torch.tensor([[0, 1], [1, 0]])\n",
    "v =  torch.tensor([3, 5.], requires_grad=True, dtype=torch.float32)\n",
    "s = torch.sparse_coo_tensor(i, v, (2, 2), requires_grad=True)\n",
    "\n",
    "# crow_indices = torch.tensor([0, 2, 4])\n",
    "# col_indices = torch.tensor([0, 1, 0, 1])\n",
    "# values = torch.tensor([1., 2, 3, 4], requires_grad=True)\n",
    "# s = torch.sparse_csr_tensor(crow_indices, col_indices, values)\n",
    "\n",
    "v2 = torch.randn((2,1))\n",
    "# s = s.to_dense()\n",
    "loss = torch.sparse.mm(s, v2).sum()\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "771f4d72-adef-41ed-85bc-2217f268ecb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.sparse_coo torch.strided\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 3).to_sparse().requires_grad_(True)\n",
    "b = torch.randn(2, 3, requires_grad=True)\n",
    "print(a.layout, b.layout)\n",
    "\n",
    "torch.sparse.mm(a,b.t()).sum().backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alg3.12",
   "language": "python",
   "name": "alg3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
