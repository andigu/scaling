{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "22512fbe-b2b8-44d9-bb0c-f453c7fe98d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"src/\")\n",
    "\n",
    "from simulate import Algorithm, BivariateBicycle, NoiseModel, CodeFactory, StimSimulator, LogicalCircuit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "from torch_geometric.data import Data\n",
    "import math\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "class BivariateBicycleDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    PyTorch IterableDataset for bivariate bicycle code error correction data.\n",
    "    \n",
    "    Generates batches of bivariate bicycle code detector measurements and corresponding \n",
    "    logical error labels for training graph neural network decoders.\n",
    "    \n",
    "    Supports 3-stage curriculum learning with automatic p adjustment.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, l=6, m=6, rounds=10, p=2.0, batch_size=32, \n",
    "                 stage_manager=None, num_workers=8, global_step_offset=0, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            l: BB code parameter l\n",
    "            m: BB code parameter m\n",
    "            rounds: Number of syndrome rounds\n",
    "            p: Default error probability parameter (used when stage_manager is None)\n",
    "            batch_size: Batch size for generated data\n",
    "            stage_manager: StageManager instance for curriculum learning (optional)\n",
    "            num_workers: Total number of DataLoader workers (for step estimation)\n",
    "            global_step_offset: Starting global step offset (for resuming)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.default_p = p\n",
    "        self.batch_size = batch_size\n",
    "        self.l = l\n",
    "        self.m = m\n",
    "        self.rounds = rounds\n",
    "        self.stage_manager = stage_manager\n",
    "        self.num_workers = num_workers\n",
    "        self.global_step_offset = global_step_offset\n",
    "        \n",
    "        # Track local worker sample count (will be set in each worker)\n",
    "        self.local_sample_count = 0\n",
    "        \n",
    "        # Build static code graph structure (independent of rounds)\n",
    "        self._static_graph_structure = self._build_static_graph_structure()\n",
    "    \n",
    "    def get_neighborhood_size(self):\n",
    "        \"\"\"Get the total number of relation types (static + temporal).\"\"\"\n",
    "        detectors, logical_errors, (rounds, p), mt = self.generate_batch(\n",
    "                self.l, self.m, 10, 0.01, 1\n",
    "        )\n",
    "        graph = self._build_spatiotemporal_graph_structure(rounds, mt)\n",
    "        return graph.shape[1]\n",
    "    \n",
    "    def get_num_logical_qubits(self):\n",
    "        \"\"\"Get the number of logical qubits encoded by this BB code.\"\"\"\n",
    "        # logical_operators is a numpy structured array with fields: logical_id, logical_pauli, data_id, physical_pauli\n",
    "        # logical_id goes from 0 to k-1 where k is the number of logical qubits\n",
    "        logical_ops = self._static_graph_structure['metadata'].logical_operators\n",
    "        return np.max(logical_ops['logical_id']) + 1\n",
    "    \n",
    "    def _estimate_global_step(self) -> int:\n",
    "        \"\"\"Estimate current global step from worker-local sample count.\"\"\"\n",
    "        return self.global_step_offset + (self.local_sample_count * self.num_workers)\n",
    "    \n",
    "    def get_current_p(self) -> float:\n",
    "        \"\"\"Get current p value based on stage manager or default.\"\"\"\n",
    "        if self.stage_manager is not None:\n",
    "            estimated_step = self._estimate_global_step()\n",
    "            return self.stage_manager.get_current_p(estimated_step)\n",
    "        else:\n",
    "            return self.default_p\n",
    "    \n",
    "    def _build_static_graph_structure(self):\n",
    "        \"\"\"Build the static BB code graph structure (independent of rounds).\"\"\"\n",
    "        # Create BB code metadata\n",
    "        cf = CodeFactory(BivariateBicycle, {'l': self.l, 'm': self.m})\n",
    "        metadata = cf().metadata\n",
    "        \n",
    "        # Build tanner graph - this creates edges between checks and data\n",
    "        tanner = pd.DataFrame(metadata.tanner)\n",
    "        \n",
    "        # Create check-to-check graph as in bb-test.ipynb\n",
    "        c2c = tanner.merge(tanner, on='data_id')[['check_id_x', 'check_id_y', 'edge_type_x', 'edge_type_y']]\n",
    "        all_edge_types = []\n",
    "        graph_edges = []\n",
    "        \n",
    "        for (c1, c2), df in c2c.groupby(['check_id_x', 'check_id_y']):\n",
    "            edge_types = df[['edge_type_x', 'edge_type_y']].to_numpy()\n",
    "            edge_types = frozenset([tuple(map(int, x)) for x in edge_types])\n",
    "            if edge_types not in all_edge_types:\n",
    "                all_edge_types.append(edge_types)\n",
    "            c1, c2 = map(int, (c1, c2))\n",
    "            graph_edges.append((c1, c2, all_edge_types.index(edge_types)))\n",
    "        \n",
    "        graph_df = pd.DataFrame(graph_edges, columns=['syndrome_id', 'neighbor_syndrome_id', 'edge_type'])\n",
    "        \n",
    "        return {\n",
    "            'graph_df': graph_df,\n",
    "            'all_edge_types': all_edge_types,\n",
    "            'metadata': metadata\n",
    "        }\n",
    "    \n",
    "    def _build_spatiotemporal_graph_structure(self, rounds, mt):\n",
    "        \"\"\"Build the spatiotemporal graph structure using the static graph and temporal connections.\n",
    "        \n",
    "        Args:\n",
    "            rounds: Number of error correction rounds\n",
    "            mt: MeasurementTracker from the LogicalCircuit (contains detector metadata)\n",
    "        \"\"\"\n",
    "        # Use pre-computed static structure\n",
    "        graph_df = self._static_graph_structure['graph_df']\n",
    "        all_edge_types = self._static_graph_structure['all_edge_types']\n",
    "        \n",
    "        # Get actual detector metadata from measurement tracker\n",
    "        det = pd.DataFrame(mt.detectors)\n",
    "        \n",
    "        t0 = det[det['time'] == 1].copy()\n",
    "        t0['time'] = 0\n",
    "        t0['detector_id'] = -1\n",
    "        tf = det[det['time'] == det['time'].max()]\n",
    "        opposite_basis = t0[~t0['syndrome_id'].isin(tf['syndrome_id'].values)].copy()\n",
    "        opposite_basis['time'] = det['time'].max()\n",
    "        tf2 = t0.copy()\n",
    "        tf2['time'] = det['time'].max()+1\n",
    "        det = pd.concat([t0, det, opposite_basis, tf2]).sort_values(by=['time', 'syndrome_id']).reset_index(drop=True)\n",
    "        \n",
    "        graph_df = self._static_graph_structure['graph_df']\n",
    "        all_edge_types = self._static_graph_structure['all_edge_types']\n",
    "        det_graph = det.merge(graph_df, on='syndrome_id').merge(\n",
    "            det, left_on='neighbor_syndrome_id', right_on='syndrome_id', suffixes=('', '_nb')\n",
    "        )\n",
    "\n",
    "        # Filter temporal connections (allow connections within 1 time step)\n",
    "        det_graph = det_graph[np.abs(det_graph['time'] - det_graph['time_nb']) <= 1]\n",
    "\n",
    "        # Encode temporal information in edge types\n",
    "        dt = det_graph['time_nb'] - det_graph['time'] + 1  # Maps [-1, 0, 1] to [0, 1, 2]\n",
    "        det_graph['final_edge_type'] = dt * len(all_edge_types) + det_graph['edge_type']\n",
    "\n",
    "        actual_graph = det_graph[det_graph['detector_id'] != -1]\n",
    "        actual_graph = actual_graph.sort_values(['detector_id', 'final_edge_type'])\n",
    "        graph = actual_graph[['detector_id', 'detector_id_nb', 'final_edge_type']].to_numpy()\n",
    "        _, counts = np.unique(graph[...,0], return_counts=True)\n",
    "        assert np.all(counts == counts[0])\n",
    "        nbrhood_size = counts[0]\n",
    "        graph = graph.reshape((-1, nbrhood_size, 3))        \n",
    "        return graph\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_batch(l, m, rounds, p, batch_size):\n",
    "        \"\"\"Generate a single batch of bivariate bicycle code data.\"\"\"\n",
    "        \n",
    "        # Create BB code circuit\n",
    "        alg = Algorithm.build_memory(cycles=rounds)\n",
    "        cf = CodeFactory(BivariateBicycle, {'l': l, 'm': m})\n",
    "        lc, _, mt = LogicalCircuit.from_algorithm(alg, cf)\n",
    "        noise_model = NoiseModel.get_scaled_noise_model(p).without_loss()\n",
    "        sim = StimSimulator(alg, noise_model, cf, seed=np.random.randint(0, 2**48))\n",
    "        \n",
    "        # Sample syndrome data\n",
    "        results = sim.sample(shots=batch_size)\n",
    "        detectors = results.detectors\n",
    "        logical_errors = results.logical_errors\n",
    "        \n",
    "        return detectors, logical_errors, (rounds, p), mt\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Generate infinite stream of bivariate bicycle code data batches.\"\"\"\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is not None:\n",
    "            worker_id = worker_info.id if hasattr(worker_info, 'id') else 'unknown'\n",
    "        else: \n",
    "            worker_id = 'main'\n",
    "            np.random.seed(0)\n",
    "        \n",
    "        # Reset local sample count for this worker\n",
    "        self.local_sample_count = 0\n",
    "        \n",
    "        while True:\n",
    "            # Calculate current p based on estimated global step\n",
    "            current_p = self.get_current_p()\n",
    "            \n",
    "            # Generate batch with current curriculum p value\n",
    "            detectors, logical_errors, (rounds, p), mt = self.generate_batch(\n",
    "                self.l, self.m, self.rounds, current_p, self.batch_size\n",
    "            )\n",
    "            synd_id = mt.detectors['syndrome_id']\n",
    "            detectors = detectors * (synd_id.max()+1) + synd_id[None,:]\n",
    "            # No longer yielding graph since it's static and registered in the model\n",
    "            yield detectors.astype(np.int32), np.squeeze(logical_errors.astype(np.float32), axis=1), (rounds, p)\n",
    "\n",
    "            # Increment local sample count after generating batch\n",
    "            self.local_sample_count += 1\n",
    "    \n",
    "    def get_graph(self):\n",
    "        # Generate batch with current curriculum p value\n",
    "        detectors, logical_errors, (rounds, p), mt = self.generate_batch(\n",
    "            self.l, self.m, self.rounds, 1.0, self.batch_size\n",
    "        )\n",
    "        synd_id = mt.detectors['detector_id']\n",
    "        detectors = detectors * (synd_id.max()+1) + synd_id[None,:]\n",
    "        graph = self._build_spatiotemporal_graph_structure(rounds, mt)\n",
    "        return graph\n",
    "    \n",
    "    def get_num_embeddings(self):\n",
    "        detectors, logical_errors, (rounds, p), mt = self.generate_batch(\n",
    "                self.l, self.m, self.rounds, 1.0, self.batch_size\n",
    "            )\n",
    "        synd_id = mt.detectors['syndrome_id']\n",
    "        return 2*(synd_id.max()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c81d0b76-0650-41fa-a3d5-41e7571aba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BB2(IterableDataset):\n",
    "    \"\"\"\n",
    "    PyTorch IterableDataset for bivariate bicycle code error correction data.\n",
    "    \n",
    "    Generates batches of bivariate bicycle code detector measurements and corresponding \n",
    "    logical error labels for training graph neural network decoders.\n",
    "    \n",
    "    Supports 3-stage curriculum learning with automatic p adjustment.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, l=6, m=6, rounds=10, p=2.0, batch_size=32, \n",
    "                 stage_manager=None, num_workers=8, global_step_offset=0, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            l: BB code parameter l\n",
    "            m: BB code parameter m\n",
    "            rounds: Number of syndrome rounds\n",
    "            p: Default error probability parameter (used when stage_manager is None)\n",
    "            batch_size: Batch size for generated data\n",
    "            stage_manager: StageManager instance for curriculum learning (optional)\n",
    "            num_workers: Total number of DataLoader workers (for step estimation)\n",
    "            global_step_offset: Starting global step offset (for resuming)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.default_p = p\n",
    "        self.batch_size = batch_size\n",
    "        self.l = l\n",
    "        self.m = m\n",
    "        self.rounds = rounds\n",
    "        self.stage_manager = stage_manager\n",
    "        self.num_workers = num_workers\n",
    "        self.global_step_offset = global_step_offset\n",
    "        \n",
    "        # Track local worker sample count (will be set in each worker)\n",
    "        self.local_sample_count = 0\n",
    "        \n",
    "        # Build static code graph structure (independent of rounds)\n",
    "        self._static_graph_structure = self._build_static_graph_structure()\n",
    "    \n",
    "    def get_neighborhood_size(self):\n",
    "        \"\"\"Get the total number of relation types (static + temporal).\"\"\"\n",
    "        detectors, logical_errors, (rounds, p), mt = self.generate_batch(\n",
    "                self.l, self.m, 10, 0.01, 1\n",
    "        )\n",
    "        graph = self._build_spatiotemporal_graph_structure(rounds, mt)\n",
    "        return graph.shape[1]\n",
    "    \n",
    "    def get_num_logical_qubits(self):\n",
    "        \"\"\"Get the number of logical qubits encoded by this BB code.\"\"\"\n",
    "        # logical_operators is a numpy structured array with fields: logical_id, logical_pauli, data_id, physical_pauli\n",
    "        # logical_id goes from 0 to k-1 where k is the number of logical qubits\n",
    "        logical_ops = self._static_graph_structure['metadata'].logical_operators\n",
    "        return np.max(logical_ops['logical_id']) + 1\n",
    "    \n",
    "    def _estimate_global_step(self) -> int:\n",
    "        \"\"\"Estimate current global step from worker-local sample count.\"\"\"\n",
    "        return self.global_step_offset + (self.local_sample_count * self.num_workers)\n",
    "    \n",
    "    def get_current_p(self) -> float:\n",
    "        \"\"\"Get current p value based on stage manager or default.\"\"\"\n",
    "        if self.stage_manager is not None:\n",
    "            estimated_step = self._estimate_global_step()\n",
    "            return self.stage_manager.get_current_p(estimated_step)\n",
    "        else:\n",
    "            return self.default_p\n",
    "    \n",
    "    def _build_static_graph_structure(self):\n",
    "        \"\"\"Build the static BB code graph structure (independent of rounds).\"\"\"\n",
    "        # Create BB code metadata\n",
    "        cf = CodeFactory(BivariateBicycle, {'l': self.l, 'm': self.m})\n",
    "        metadata = cf().metadata\n",
    "        \n",
    "        # Build tanner graph - this creates edges between checks and data\n",
    "        tanner = pd.DataFrame(metadata.tanner)\n",
    "        \n",
    "        # Create check-to-check graph as in bb-test.ipynb\n",
    "        c2c = tanner.merge(tanner, on='data_id')[['check_id_x', 'check_id_y', 'edge_type_x', 'edge_type_y']]\n",
    "        all_edge_types = []\n",
    "        graph_edges = []\n",
    "        \n",
    "        for (c1, c2), df in c2c.groupby(['check_id_x', 'check_id_y']):\n",
    "            edge_types = df[['edge_type_x', 'edge_type_y']].to_numpy()\n",
    "            edge_types = frozenset([tuple(map(int, x)) for x in edge_types])\n",
    "            if edge_types not in all_edge_types:\n",
    "                all_edge_types.append(edge_types)\n",
    "            c1, c2 = map(int, (c1, c2))\n",
    "            graph_edges.append((c1, c2, all_edge_types.index(edge_types)))\n",
    "        \n",
    "        graph_df = pd.DataFrame(graph_edges, columns=['syndrome_id', 'neighbor_syndrome_id', 'edge_type'])\n",
    "        return {\n",
    "            'graph_df': graph_df.sort_values(by=['syndrome_id', 'edge_type']),\n",
    "            'all_edge_types': all_edge_types,\n",
    "            'metadata': metadata\n",
    "        }\n",
    "        \n",
    "    @staticmethod\n",
    "    def generate_batch(l, m, rounds, p, batch_size):\n",
    "        \"\"\"Generate a single batch of bivariate bicycle code data.\"\"\"\n",
    "        \n",
    "        # Create BB code circuit\n",
    "        alg = Algorithm.build_memory(cycles=rounds)\n",
    "        cf = CodeFactory(BivariateBicycle, {'l': l, 'm': m})\n",
    "        lc, _, mt = LogicalCircuit.from_algorithm(alg, cf)\n",
    "        noise_model = NoiseModel.get_scaled_noise_model(p).without_loss()\n",
    "        sim = StimSimulator(alg, noise_model, cf, seed=np.random.randint(0, 2**48))\n",
    "        \n",
    "        # Sample syndrome data\n",
    "        results = sim.sample(shots=batch_size)\n",
    "        detectors = results.detectors\n",
    "        logical_errors = results.logical_errors\n",
    "        \n",
    "        return detectors, logical_errors, (rounds, p), mt\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Generate infinite stream of bivariate bicycle code data batches.\"\"\"\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is not None:\n",
    "            worker_id = worker_info.id if hasattr(worker_info, 'id') else 'unknown'\n",
    "        else: \n",
    "            worker_id = 'main'\n",
    "            np.random.seed(0)\n",
    "        \n",
    "        # Reset local sample count for this worker\n",
    "        self.local_sample_count = 0\n",
    "        \n",
    "        while True:\n",
    "            # Calculate current p based on estimated global step\n",
    "            current_p = self.get_current_p()\n",
    "            \n",
    "            # Generate batch with current curriculum p value\n",
    "            detectors, logical_errors, (rounds, p), mt = self.generate_batch(\n",
    "                self.l, self.m, self.rounds, current_p, self.batch_size\n",
    "            )\n",
    "            synd_id = mt.detectors['syndrome_id']\n",
    "            detectors = detectors * (synd_id.max()+1) + synd_id[None,:]\n",
    "            \n",
    "            ret = np.zeros((detectors.shape[0], rounds+1, synd_id.max()+1), dtype=np.int32)\n",
    "            \n",
    "            ret[:, mt.detectors['time']-1, mt.detectors['syndrome_id']] = detectors+1\n",
    "            # No longer yielding graph since it's static and registered in the model\n",
    "            yield ret, np.squeeze(logical_errors.astype(np.float32), axis=1), (rounds, p)\n",
    "\n",
    "            # Increment local sample count after generating batch\n",
    "            self.local_sample_count += 1\n",
    "    \n",
    "    def get_graph(self):\n",
    "        # Generate batch with current curriculum p value\n",
    "        detectors, logical_errors, (rounds, p), mt = self.generate_batch(\n",
    "            self.l, self.m, self.rounds, 1.0, self.batch_size\n",
    "        )\n",
    "        synd_id = mt.detectors['detector_id']\n",
    "        detectors = detectors * (synd_id.max()+1) + synd_id[None,:]\n",
    "        graph = self._build_spatiotemporal_graph_structure(rounds, mt)\n",
    "        return graph\n",
    "    \n",
    "    def get_num_embeddings(self):\n",
    "        detectors, logical_errors, (rounds, p), mt = self.generate_batch(\n",
    "                self.l, self.m, self.rounds, 1.0, self.batch_size\n",
    "            )\n",
    "        synd_id = mt.detectors['syndrome_id']\n",
    "        return 2*(synd_id.max()+1)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f775f9ee-b064-4471-917c-4baa36147da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_torch_function(func, *args, warmup_runs=5, num_runs=20, **kwargs):\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    # --- Warm-up Phase ---\n",
    "    for _ in range(warmup_runs):\n",
    "        _ = func(*args, **kwargs)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # --- Time Measurement ---\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "    timings = []\n",
    "    for _ in range(num_runs):\n",
    "        start_event.record()\n",
    "        _ = func(*args, **kwargs)\n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "        timings.append(start_event.elapsed_time(end_event))\n",
    "    \n",
    "    average_time_ms = sum(timings) / len(timings)\n",
    "\n",
    "    # --- Memory Measurement ---\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    _ = func(*args, **kwargs)\n",
    "    torch.cuda.synchronize()\n",
    "    peak_memory_bytes = torch.cuda.max_memory_allocated(device)\n",
    "    peak_memory_mb = peak_memory_bytes / (1024 * 1024)\n",
    "\n",
    "    return {\n",
    "        \"average_time_ms\": average_time_ms,\n",
    "        \"peak_memory_mb\": peak_memory_mb\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0fac6624-3877-48c6-a463-126b5e32ddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = BB2(l=6, m=6, p=1.0, rounds=6, batch_size=64)\n",
    "graph_df = ds._build_static_graph_structure()['graph_df'].copy()\n",
    "edge_types = list(sorted(set(map(tuple, graph_df.groupby('syndrome_id').agg(list)['edge_type'].tolist()))))\n",
    "edge_type_bdr = min(edge_types[1])\n",
    "graph_df.loc[graph_df['edge_type'] >= edge_type_bdr, 'edge_type'] -= edge_type_bdr\n",
    "\n",
    "graph = np.zeros((graph_df['syndrome_id'].max()+1, 22), dtype=int)\n",
    "graph[graph_df['syndrome_id'], graph_df['edge_type']] = graph_df['neighbor_syndrome_id']\n",
    "\n",
    "\n",
    "embedding_dim = 128\n",
    "emb = nn.Embedding(ds.get_num_embeddings(), embedding_dim=embedding_dim).cuda()\n",
    "lin = nn.Linear(graph.shape[1]*embedding_dim, 3*embedding_dim).cuda()\n",
    "\n",
    "x, y, _ = next(iter(ds))\n",
    "x, y = torch.from_numpy(x).int().cuda(), torch.from_numpy(y)\n",
    "x = emb(x)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "fb749767-d4e0-4f49-8fad-af15b01833f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1, x2 = torch.chunk(lin(x[:,:,graph,:].flatten(start_dim=-2)), 3, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "962f2ee1-10f8-434f-bb33-4cdb2ac51e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'average_time_ms': 11.615856666564941, 'peak_memory_mb': 7635.63818359375}\n"
     ]
    }
   ],
   "source": [
    "def method1():\n",
    "    x0, x1, x2 = torch.chunk(lin(x[:,:,graph,:].flatten(start_dim=-2)), 3, dim=-1)\n",
    "    xout = F.pad(x0[:,:-1], (0,0,0,0,1,0,0,0)) + x1 + F.pad(x2[:,1:], (0,0,0,0,0,1,0,0))\n",
    "profiling_results = profile_torch_function(\n",
    "    method1, warmup_runs=10, num_runs=100\n",
    ")\n",
    "print(profiling_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9fe0053b-12e0-4442-bf09-8f33482e4253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=8448, out_features=128, bias=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "ds = BivariateBicycleDataset(l=6, m=6, p=1.0, rounds=6, batch_size=64)\n",
    "embedding_dim = 128\n",
    "emb = nn.Embedding(ds.get_num_embeddings(), embedding_dim=embedding_dim).cuda()\n",
    "lin = nn.Linear(ds.get_neighborhood_size()*embedding_dim, embedding_dim).cuda()\n",
    "print(lin)\n",
    "x, y, _ = next(iter(ds))\n",
    "x, y = torch.from_numpy(x).int().cuda(), torch.from_numpy(y)\n",
    "x = emb(x)\n",
    "x2 = F.pad(x, (0,0,0,1,0,0))\n",
    "out = lin(x2[:, ds.get_graph()[...,1]].flatten(start_dim=2))\n",
    "def method2():\n",
    "    x2 = F.pad(x, (0,0,0,1,0,0))\n",
    "    out = lin(x2[:, ds.get_graph()[...,1]].flatten(start_dim=2))\n",
    "\n",
    "profiling_results = profile_torch_function(\n",
    "    method2, warmup_runs=10, num_runs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "617caaff-e06d-497e-9d42-fa27d1527639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'average_time_ms': 108.21879806518555, 'peak_memory_mb': 8185.26025390625}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiling_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scale",
   "language": "python",
   "name": "scale"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
